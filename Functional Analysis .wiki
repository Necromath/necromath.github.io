Here are my written solutions to Chapter 5 of Folland:

= [[Functional Analysis Notes]] =

Exercise 1: If $X$ is a normed vector space over $K (= \mathbb{R} \textrm{or} \mathbb{C} )$, then addition and multiplication are continuous from $X \times X$ and $K \times X$ to $X$. Moreover, the norm is continuous from $X$ to $[0,\infty)$; in fact, $\|x\|-\|y\| \leq \|x-y\|$. 

Proof:
To prove continuity of addition we use a $\delta - \epsilon $ proof. Let $x,x_0,y,y_0 \in X$ be such that $\|(x,y)-(x_0,y_0)\|=\|((x-x_0,y-y_0))\| < \delta = \frac{\epsilon}{2}.$ From the definition of product norm, this is simply $\textrm{max}(\|(x-x_0)\|,\|(y-y_0)\|).$ Then we see that under the map $+:X\times X \rightarrow X$ we have that 

$ \begin{equation} \begin{split}  \|(x+y)-(x_0+y_0)\| &= \|(x-x_0) + (y-y_0)\| \\ &\leq \|x-x_0\| + \|y-y_0\| \\  &< \frac{\epsilon}{2} + \frac{\epsilon}{2} \\ &= \epsilon \end{split}\end{equation}$ 

Thus, addition is continuous in $X$. 

Next we prove continuity of multiplication. Let $\lambda$ be an element of our field $K$ and let $x$ be an element of $X$. The goal is to show that if $\|(\lambda,x)-(\psi,x_0)\| < \delta.$ Then $\|\lambda x - \psi x_0 \| < \epsilon $. Usually whenever I try to work out these sorts of inequalities, I never really see it until I see a cheesed out answer. So here is the cheesed out answer. Consider 

$\begin{equation} \begin{split}    \| \lambda x - \psi x_0 \| &= \|\lambda x -\lambda x_0 + \lambda x_0 - \psi x_0\| \\ &\leq \| \lambda x -\lambda x_0 \| + \| \lambda x_0 - \psi x_0 \| \\ &= |\lambda| \|x-x_0\|+ |\lambda - \psi| \|x_0\| \end{split} \end{equation}$. 

What should one do in this situation? Why simply set $\delta = \textrm{min} (1, \frac{\epsilon}{3\|x\|}, \frac{\epsilon}{3|\lambda|}, \frac{\epsilon}{3} ) $ Then we know that $|\lambda - \psi| < 1$. So then 

$ \begin{equation} \begin{split} |\lambda| \|x-x_0\|+ |\lambda - \psi| \|x_0\| & \leq |\lambda - \psi| \|x\| + |\psi| \|x-x_0\| + |\lambda - \psi | \|x-x_0\| \\ &< \frac{\epsilon}{3} + \frac{\epsilon}{3} + \frac{\epsilon}{3} \\ &< \epsilon \end{split} \end{equation}$

Thus, multiplication is continuous. 

To show that the norm $\| \bullet \| : X \rightarrow [0,\infty)$ is continuous, we must first show that $\| x + y \| \leq \|x\| + \|y\|$. Observe that: 

$ \|x\| = \|x-y+y\| \leq \|x-y\| + \|y\| $. Implying that $\|x\| -\|y\| \leq \|x-y\|$. This is the reverse triangle inequality. 

Now to show continuity: Let x,y be elements of $X$ and let $\delta = \|x-y\| $, then $| \|x\| - \|y\| | \leq \|x-y\| < \epsilon $, then $\|x-y\| < \epsilon$ and we see that norm is continuous. 

Exercise 4: If $X,Y$ are normed vector spaces, the map $(T,x) \rightarrow Tx $ is continuous from  $L(X,Y) \times X $ to $Y$. (If $T_n \rightarrow T$ and $x_n \rightarrow x$ then $T_n x_n \rightarrow Tx$)

Proof:
To prove continuity of the operation, we must show that $\|T_nx_n - Tx\| \rightarrow 0$. Since $x_n$, it is bounded. So there exists some $C>0$ such that $\|x_n\| \leq C , \forall n \in \mathbb{N}$. 

Then we have that 
$\begin{equation} \begin{split}  \|T_nx_n-Tx\|  & =  \|T_nx_n-Tx_n +Tx_n -Tx\| \\  &= \|T_nxn_n - Tx_n\| + \|Tx_n -Tx_\| \\  &= \|x_n\| \|T_n-T\|  + \|T\| \|x_n-x\| \\  & \leq C \|T_n-T\| + \|T\| \|x_n -x\| \rightarrow 0,  \textrm{as } n \rightarrow \infty \end{split}\end{equation} $ 

Thus, we see that the map is continuous.

Exercise 7: Let $X$ be a Banach space.

a) If $T \in L(X,X)$ and $\|I-T\| < 1$, where $I$ is the identity operator, then $T$ is invertible; in fact, the series $\Sigma _0 ^\infty (I-T)^n $ converges in $L(X,X)$ to $T^{-1}$.
b) If $T\in L(X,X)$ is invertible and $\|S-T\| < \|T^{-1}\|^{-1} $, then $S$ is invertible. Thus the set of invertible operators is open in $L(X,X)$.

Proof: 

(1) First we observe that $\|(I-T)^n \| \leq \| I -T\| ^n$. Since we know from our hypothesis that $\|I-T\| < 1$, we can conclude that $\Sigma_{n=0}^\infty \|(I-T)^n\| \leq \Sigma_{n=0}^\infty \|I-T\|^n$ is absolutely convergent as $n\rightarrow \infty$. Thus $L(X,X)$ is a Banach space and this series converges to some $K \in L(X,X)$. Then we can see that, via induction, that:
$\begin{split} KT &=(I +(I-T)+ ... + (I-T)^{n-1})T \\ &=  I - (I-T)^n \\ &= T(I+(I-T)+...+ (I-T)^{n-1}) = TK \end{split}$

So then we see that as $n \rightarrow \infty$ we see that $KT$ simply becomes the identity operator. Thus proving invertability of $T$.
Observe that $ \begin{equation}\begin{split} \|I-T^{-1}S\|=\|T^{-1}S - I\| &= \|T^{-1}S - T^{-1}T \| \\  &\leq \|T^{-1}\| \|S-T\| \\ &< \|T^{-1}\| \|T^{-1}\|^{-1} = 1 \end{split}\end{equation}$. 

The last inequality comes from our given.

(2) From part a. We can conclude that $T^{-1}S$ is invertible and has an operator $K \in L(X,X)$ such that $KT^{-1}S = T^{-1}SK = I$. Then we can see that $KT^{-1}$ is an inverse of $S$. So every element of a $\frac{1}{\|T^{-1}\|}-\textrm{ball }$ around $T$ is invertible, and thus the set of invertible operators is open in $L(X,X)$.  

Exercise 9: Let $C^k([0,1])$ be the space of functions on $[0,1]$ possesing continuous derivatives up to order $k$ on $[0,1]$, inclduing one sided derivatives at the endpoints.

a) If $f \in C([0,1])$, then $f\in C^k([0,1])$ iff $f$ is $k$ times continuously differentiable on $(0,1)$ and $lim_{x \searrow 0} f^{(j)}(x)$ and $lim_{x \nearrow 1} f^{(j)}(x)$ exist for $j \leq k$. 
b) $\| f\| = \Sigma_0^k \|f^{(j)} \|_u $ is a norm on $C^k([0,1])$ that makes it into a Banach space. 
   
Proof: 

(1) The if direction is trivial. The definition for $f$ to be an element of $C^k([0,1])$ automatically satisfies the conditions on the $(0,1)$ and at the end points. Working backwards: If $f$ is $k$ times continuously differentiable in $(0,1)$ and $lim_{x \searrow 0} f^{(j)}(x)$, $lim_{x \nearrow 1} f^{(j)}(x)$ exist for $j \leq k$. The mean value theorem tells us that for every $c\in(0,1)$ there exists $d \in (0,z)$ such that: $\frac{f^{(j-1)}(x) - f^{(j-1)} (0)} {x} = f^{(j)} (c) $ Taking the limit $x \searrow 0$, we get that $f^{(j)} (0) = lim _{c \searrow 0} f^{(j)} (c) $.  This can be done similarly for $x \nearrow 1$. Then, since we have that $f$ is continuously differentiable at every point, we conclude $f\in C^k([0,1])$

(2) First we show that $ \| f\| = \Sigma_0^k \|f^{(j)} \|_u $ is a norm on $C^k([0,1])$ Obviously, $\| f\| \geq 0 $ for all $f\in C^k([0,1])$. If $f\in C^k([0,1])$ is nonzero, then $\|f^{(0)}\|_u > 0$ and so $\|f\|>0$. Now, let $f,g \in C^k([0,1])$ and $\lambda \in \mathbb{R}$. Then: $  \|\lambda f \| = \Sigma_{j=0}^k \|\lambda f^{(j)} \|_u   = \Sigma_{j=0}^k |\lambda | \|f^{(j)} \|_u $ and $\|f+g\| = \Sigma _{j=0} ^k \|f^{(j)} + g^{(j)} \|_u \leq \Sigma_{j=0}^k \|f^{(j)}\|_u + \Sigma_{j=0}^k \|g^{(j)}\|_u = \|f+\|g\|$

Thus, we have shown this is a norm. Now we turn to show this is a Banach space.

Proceeding by induction, we consider the base case with $C^0([0,1])$ and the norm $\| f\|_u = \textrm{sup}_{[0,1]} |f|$. Let ${f_n}$ be a Cauchy sequence in $C^0([0,1])$. Then:

$\begin{split} \|f-f_n\|_u & = \textrm{sup}_{[0,1]}|f(x)-f_n(x)| \\ &= \textrm{sup}_{[0,1]} \textrm{lim}_{m \rightarrow \infty} |f_n(x)-f_m(x) | \\ & \leq \textrm{lim  inf}_{m \rightarrow \infty} \textrm{sup}_{[0,1]} | f_n(x)-f_m(x) | \\ &\rightarrow 0, \textrm{as  } n \rightarrow \infty  \end{split} $

Now let us suppose that this holds for $j \leq k$, we wish to show that this holds for $j=k+1$. Let ${f_n}$ be a Cauchy sequence in  $C^{k+1}([0,1])$. Then:

$\Sigma_0^k \|f_n^{(j)}-f_m^{j}\|_u < \Sigma_0^{k+1} \|f_n^{(j)}-f_m^{j}\|_u = \|f_n -f_m\| $

So then this is also a Cauchy sequence in $C^k([0,1])$ and converges to some $f \in C^k([0,1])$. So then our space is complete.

Exercise 10: Let $L^1_k([0,1])$ be the space of all $f \in C^{k-1}([0,1])$ such that $f^{(k-1)}$ is absolutely continuous on $[0,1]$. Then $\|f\| = \Sigma_0^k \int _0 ^1 | f^{(j)}(x)| dx$ is a norm on $L^1_k([0,1])$ that makes it into a Banach space.

Proof: First we check that this is norm. First and foremost, this is obviously nonnegative. Scalars can be removed safely as well. So we really just need to show that this satisfies the triangle inequality. That is, if $f,g \in L^1_k([0,1])$, then $\begin{split} \|x+y\|= \Sigma_0^k \int _0 ^1 | f^{(j)}(x) + g^{(j)}(x)| dx  & \leq  \Sigma_0^k(\int_0^1|f^{(j)}(x)|dx + \int_0^1|g^{(j)}(x)|dx) \\ &= \Sigma_0^k \int _0 ^1 | f^{(j)}(x)| dx + \Sigma_0^k \int _0 ^1 | g^{(j)}(x)| dx \\ &= \|x\| + \|y\|  \end{split}$

Thus, this is in fact a norm. Now we need to show this is a Banach space. 

A small adaptation of part b from the previous exercise will prove this is a Banach space. (incomplete)

Exercise 11: Hoelder continuous.

Proof: I'll only show triangle inequality, others are trivial.

Suppose we have $f,g \in \Lambda_\alpha ([0,1])$. Then:

$\begin{split} \|f+g\|_{\Lambda_\alpha} &= |f(0) + g(0) | + \textrm{sup}_{x,y\in[0,1], x\neq y} \frac{|f+g(x)-f+g(y)|}{|x-y|^\alpha} \\ & \leq |f(0)| +|g(0)| +  \textrm{sup}_{x,y\in[0,1], x\neq y} \frac{|f(x)-f(y)|}{|x-y|^\alpha} +  \textrm{sup}_{x,y\in[0,1], x\neq y} \frac{|g(x)-g(y)|}{|x-y|^\alpha} = \|f\|_{\Lambda_\alpha} + \|g\|_{\Lambda_\alpha} \end{split} $

Thus, $\| \bullet \|_{\Lambda_\alpha}$ is a norm. 

Now we turn to show that $\Lambda_\alpha ([0,1])$ into a Banach space. 

Exercise 12: Let $X$ be a normed vector space and $M$ a proper closed subspace of $X$. 
1) $\| x + M \| = \textrm{inf} {\|x+y\| : y\in M}$ is a norm on $X\setminus M$.
2) For any $\epsilon > 0 $ there exists $x\in X$ such that $\| x\| = 1$ and $\|x + M \| \geq 1 - \epsilon$.
3) The projection map $\pi(x) = x+ M $ from $X $ to $X \setminus M$ has norm 1.
4) If $X$ is complete, so is $X\setminus M$
5) The topology defined by the quotient norm is the quotient topology as in Exercise 28 of 4.2

Proof: 
1) Clearly $0+M = M$, implying that $0 \in M$ and so $\| 0 + M \| = \textrm{inf}\{\|0+y \| : y \in M\} = 0$. Next, we observe that

$ \begin{split} \| \lambda x + M \| &= \textrm{inf}_{y\in M} \| \lambda x +y \| \\ &= | \lambda | \textrm{inf}_{y\in M} \| x + \frac{y}{a} \| \\ &= |\lambda | \textrm{inf}_{ y \in M} \|x + y\| \\ &= | \lambda | \|x+ M\| \end{split}$

Also:

$ \begin {split} \|x+y+M\| &= \textrm{inf}_{z\in M} \|x+y+z\|  \\  & \leq \textrm{inf}_{z\in M} \|x + \frac{z}{2} \| + \textrm{inf}_{z \in M } \| y + \frac{z}{2} \|   \\  & = \textrm{inf}_{z \in M} \|x + z\| + \textrm{inf}_{z \in M} \| y + z\|  \\ & = \|x + M \| + \|y + M \| \end{split}$

So $\| x + M \| $ is a norm on $X \setminus M$.

2) Suppose that for $x \in X \setminus M$ we have that $\|x+M\|  \leq \frac{\|x + M}{1-\epsilon}$. Then that means we have a $y\in M$ such that $\|x+y\| \leq \frac{\|x + M}{1-\epsilon}=\frac{(x+y)+M}{1-\epsilon}$. After switching fractions, we see that $1-\epsilon \leq \| \frac{x+y}{\|x+y\|}+M\|= \| 1 + M \| $. 
   
3) Observe that the projection map has norm $\| \pi \| = \textrm{sup} \{ \frac{\| \pi x \|}{\|x\|} : x \neq 0 \} \leq 1$. However, the previous part tells us that$\| \pi = \textrm{sup} _P{\|x\|=1} \|x+M\| \geq 1-\epsilon\| $. Since $\epsilon$ is arbitrary, we see that $\| \pi\| = 1 $.

4) Suppose that $X$ is complete and that $\sum_{n=1}^{\infty}(x_n + M)$ is an absolutely convergent series in $X \setminus M $. Theorem 5.1 lets us find a $y\in M$ such that the $\sum_{n=1}{\infty} \|x_n + y\| < \sum_{n=1}{\infty}(\|x_n + M \| + \frac{1}{2^n}) < \infty $. 

So then the sequence $\{x_n +y \}_{n=1}^{\infty}$ is a sequence that is absolutely convergent as a series. From completeness of $X$, this converges to an element of $X$, say $x$. As a series in $ X\setminus M$, this absolutely converges to $ x +M $. 
   
5) To be continued... 
   
Exericse 15:Suppose that $X$ and $Y$ are normed vector spaces and $T\in L(X,Y).$l Let $N(T)= \{x\in X : Tx=0 \} $ 
a) $N(T)$ is a closed subspace of $X$. 
b) There is a unique $ S \in L(X \setminus N(T), Y)$ such that $T = S \circ \pi$ where $\pi : X \rightarrow X \setminus N(T)$ is the projection $\pi (x)= x + N(T)$ and $\|S\| = \|T\| $ 
   
Proof: 
1) Since $T$ is an element of $L(X,Y)$, it is continuous. Note also, that $\{0\}$ is a closed set in $Y$. So then, $N(T) = T^{-1}(\{0\})$ is a closed set in X.

2) Let $ S: X\setminus N(T) \rightarrow Y$ be the map such that $S(x + N(T)) = Tx$. This map is well defined by the fact that if $x+N(T)=y+N(T)$, then $(x-y) \ in N(T)$. So then $S(x+N(T)) = Tx = Tx +Ty - Ty = Ty + T(x-y) = T(y) = S(y+N(T))$. Further, this map is also linear since:

$S((\lambda x + \psi y) +N(T)) = T(\lambda x + \psi y) =\lambda T x + \psi T y = \lambda S(x+N(T)) + \psi S(y+N(T))$

Further, this map is also bounded below since $\|S(x+N(T))\| = \|Tx\| \leq \|x\|$

So we can conclude that $S \in L(X\setminus N(T), Y)$. 

Then we notice that $\begin{split} \|T\| &= \textrm{sup}_{x \neq 0} \frac{\|Tx\|}{\|x\|} = \textrm{sup}_{x \notin N(T)} \frac{\|S(\pi(x))\|}{\|x\|} \\ &= \textrm{sup}_{x\notin N(T)} \frac{\| S(\pi(x)) \|}{\| \pi(x) \|} \frac{\| \pi(x) \| }{\|x\|} = \textrm{sup}_{x \notin N(T)} \frac{\| S (\pi (x))\|} {\pi(x)} \textrm{sup}_{x \notin N(T)} \frac{\| \pi(x) \| } {\|x\|} = \|S\| \| \pi \| = \|S\| \end{split} $

Exercise 17: A linear functional $f$ on a normed vector space $X$ is bounded iff $f^{-1} (\{0 \}) $ is closed. 

Proof: In the forwards direction: If a linear functional is bouded, then it is continuous. So then the preimage of a closed set is closed. Thus, $f^{-1}( \{0 \})$ is closed.

In the reverse direction: Let $K = f^{-1}(\{0 \})$ be a closed subset of $X$. Using the previous exercise, we can find an $x \in X$ such that it has norm 1 and $\| x+ K ) \| \geq \frac{1}{2} $. Further, for any $x \in X$, we have $y \in K$ such that

$|f(\lambda x +y)| = |\lambda| |f(x)| \leq 2 |\lambda| \frac{1}{2} |f(x)| \leq 2 |\lambda| \|x+K\| |f(x)| \leq 2 |\lambda | \|x+ \frac{y}{\lambda}\| |f(x)| = 2|f(x)| \|\lambda x +y\|$. So then $f$ is bounded.

Exercise 18: Let $X$ be a normed vector space. 
1) If $M$ is a closed subspace and $x \in X \setminus M$ then $M + \CC x$ is closed
2) Every finite dimensional subspace of $X$ is closed. 
   
Proof: 
1) If $X = M$, then $X \setminus M = \varnothing$ and the whole statement is trivial. So let $M$ be a proper closed subspace of $X$. And, if $z \

Exercise 19:

Exercise 27:

Exercise 30:

Exercise 38:

Exercise 45:

Exercise 48:

Exercise 56:

Exercise 60:

Exercise 61:


