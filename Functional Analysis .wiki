Here are my written solutions to Chapter 5 of Folland:

= [[Functional Analysis Notes]] =

Exercise 1: If $X$ is a normed vector space over $K (= \mathbb{R} \textrm{or} \mathbb{C} )$, then addition and multiplication are continuous from $X \times X$ and $K \times X$ to $X$. Moreover, the norm is continuous from $X$ to $[0,\infty)$; in fact, $\|x\|-\|y\| \leq \|x-y\|$. 

Proof:
To prove continuity of addition we use a $\delta - \epsilon $ proof. Let $x,x_0,y,y_0 \in X$ be such that $\|(x,y)-(x_0,y_0)\|=\|((x-x_0,y-y_0))\| < \delta = \frac{\epsilon}{2}.$ From the definition of product norm, this is simply $\textrm{max}(\|(x-x_0)\|,\|(y-y_0)\|).$ Then we see that under the map $+:X\times X \rightarrow X$ we have that 

$ \begin{equation} \begin{split}  \|(x+y)-(x_0+y_0)\| &= \|(x-x_0) + (y-y_0)\| \\ &\leq \|x-x_0\| + \|y-y_0\| \\  &< \frac{\epsilon}{2} + \frac{\epsilon}{2} \\ &= \epsilon \end{split}\end{equation}$ 

Thus, addition is continuous in $X$. 

Next we prove continuity of multiplication. Let $\lambda$ be an element of our field $K$ and let $x$ be an element of $X$. The goal is to show that if $\|(\lambda,x)-(\psi,x_0)\| < \delta.$ Then $\|\lambda x - \psi x_0 \| < \epsilon $. Usually whenever I try to work out these sorts of inequalities, I never really see it until I see a cheesed out answer. So here is the cheesed out answer. Consider 

$\begin{equation} \begin{split}    \| \lambda x - \psi x_0 \| &= \|\lambda x -\lambda x_0 + \lambda x_0 - \psi x_0\| \\ &\leq \| \lambda x -\lambda x_0 \| + \| \lambda x_0 - \psi x_0 \| \\ &= |\lambda| \|x-x_0\|+ |\lambda - \psi| \|x_0\| \end{split} \end{equation}$. 

What should one do in this situation? Why simply set $\delta = \textrm{min} (1, \frac{\epsilon}{3\|x\|}, \frac{\epsilon}{3|\lambda|}, \frac{\epsilon}{3} ) $ Then we know that $|\lambda - \psi| < 1$. So then 

$ \begin{equation} \begin{split} |\lambda| \|x-x_0\|+ |\lambda - \psi| \|x_0\| & \leq |\lambda - \psi| \|x\| + |\psi| \|x-x_0\| + |\lambda - \psi | \|x-x_0\| \\ &< \frac{\epsilon}{3} + \frac{\epsilon}{3} + \frac{\epsilon}{3} \\ &< \epsilon \end{split} \end{equation}$

Thus, multiplication is continuous. 

To show that the norm $\| \bullet \| : X \rightarrow [0,\infty)$ is continuous, we must first show that $\| x + y \| \leq \|x\| + \|y\|$. Observe that: 

$ \|x\| = \|x-y+y\| \leq \|x-y\| + \|y\| $. Implying that $\|x\| -\|y\| \leq \|x-y\|$. This is the reverse triangle inequality. 

Now to show continuity: Let x,y be elements of $X$ and let $\delta = \|x-y\| $, then $| \|x\| - \|y\| | \leq \|x-y\| < \epsilon $, then $\|x-y\| < \epsilon$ and we see that norm is continuous. 


Exercise 4: If $X,Y$ are normed vector spaces, the map $(T,x) \rightarrow Tx $ is continuous from  $L(X,Y) \times X $ to $Y$. (If $T_n \rightarrow T$ and $x_n \rightarrow x$ then $T_n x_n \rightarrow Tx$)

Proof:
To prove continuity of the operation, we must show that $\|T_nx_n - Tx\| \rightarrow 0$. Since $x_n$, it is bounded. So there exists some $C>0$ such that $\|x_n\| \leq C , \forall n \in \mathbb{N}$. 

Then we have that 
$\begin{equation} \begin{split}  \|T_nx_n-Tx\|  & =  \|T_nx_n-Tx_n +Tx_n -Tx\| \\  &= \|T_nxn_n - Tx_n\| + \|Tx_n -Tx_\| \\  &= \|x_n\| \|T_n-T\|  + \|T\| \|x_n-x\| \\  & \leq C \|T_n-T\| + \|T\| \|x_n -x\| \rightarrow 0,  \textrm{as } n \rightarrow \infty \end{split}\end{equation} $ 

Thus, we see that the map is continuous.

Exercise 7: Let $X$ be a Banach space.

a) If $T \in L(X,X)$ and $\|I-T\| < 1$, where $I$ is the identity operator, then $T$ is invertible; in fact, the series $\Sigma _0 ^\infty (I-T)^n $ converges in $L(X,X)$ to $T^{-1}$.
b) If $T\in L(X,X)$ is invertible and $\|S-T\| < \|T^{-1}\|^{-1} $, then $S$ is invertible. Thus the set of invertible operators is open in $L(X,X)$.

Proof: 

(1) First we observe that $\|(I-T)^n \| \leq \| I -T\| ^n$. Since we know from our hypothesis that $\|I-T\| < 1$, we can conclude that $\Sigma_{n=0}^\infty \|(I-T)^n\| \leq \Sigma_{n=0}^\infty \|I-T\|^n$ is absolutely convergent as $n\rightarrow \infty$. Thus $L(X,X)$ is a Banach space and this series converges to some $K \in L(X,X)$. Then we can see that, via induction, that:
$\begin{split} KT &=(I +(I-T)+ ... + (I-T)^{n-1})T \\ &=  I - (I-T)^n \\ &= T(I+(I-T)+...+ (I-T)^{n-1}) = TK \end{split}$

So then we see that as $n \rightarrow \infty$ we see that $KT$ simply becomes the identity operator. Thus proving invertability of $T$.
Observe that $ \begin{equation}\begin{split} \|I-T^{-1}S\|=\|T^{-1}S - I\| &= \|T^{-1}S - T^{-1}T \| \\  &\leq \|T^{-1}\| \|S-T\| \\ &< \|T^{-1}\| \|T^{-1}\|^{-1} = 1 \end{split}\end{equation}$. 

The last inequality comes from our given.

(2) From part a. We can conclude that $T^{-1}S$ is invertible and has an operator $K \in L(X,X)$ such that $KT^{-1}S = T^{-1}SK = I$. Then we can see that $KT^{-1}$ is an inverse of $S$. So every element of a $\frac{1}{\|T^{-1}\|}-\textrm{ball }$ around $T$ is invertible, and thus the set of invertible operators is open in $L(X,X)$.  

Exercise 9: Let $C^k([0,1])$ be the space of functions on $[0,1]$ possesing continuous derivatives up to order $k$ on $[0,1]$, inclduing one sided derivatives at the endpoints.

a) If $f \in C([0,1])$, then $f\in C^k([0,1])$ iff $f$ is $k$ times continuously differentiable on $(0,1)$ and $lim_{x \searrow 0} f^{(j)}(x)$ and $lim_{x \nearrow 1} f^{(j)}(x)$ exist for $j \leq k$. 
b) $\| f\| = \Sigma_0^k \|f^{(j)} \|_u $ is a norm on $C^k([0,1])$ that makes it into a Banach space. 
   
Proof: 

(1) The if direction is trivial. The definition for $f$ to be an element of $C^k([0,1])$ automatically satisfies the conditions on the $(0,1)$ and at the end points. Working backwards: If $f$ is $k$ times continuously differentiable in $(0,1)$ and $lim_{x \searrow 0} f^{(j)}(x)$, $lim_{x \nearrow 1} f^{(j)}(x)$ exist for $j \leq k$. The mean value theorem tells us that for every $c\in(0,1)$ there exists $d \in (0,z)$ such that: $\frac{f^{(j-1)}(x) - f^{(j-1)} (0)} {x} = f^{(j)} (c) $ Taking the limit $x \searrow 0$, we get that $f^{(j)} (0) = lim _{c \searrow 0} f^{(j)} (c) $.  This can be done similarly for $x \nearrow 1$. Then, since we have that $f$ is continuously differentiable at every point, we conclude $f\in C^k([0,1])$

(2) First we show that $ \| f\| = \Sigma_0^k \|f^{(j)} \|_u $ is a norm on $C^k([0,1])$ Obviously, $\| f\| \geq 0 $ for all $f\in C^k([0,1])$. If $f\in C^k([0,1])$ is nonzero, then $\|f^{(0)}\|_u > 0$ and so $\|f\|>0$. Now, let $f,g \in C^k([0,1])$ and $\lambda \in \mathbb{R}$. Then: $  \|\lambda f \| = \Sigma_{j=0}^k \|\lambda f^{(j)} \|_u   = \Sigma_{j=0}^k |\lambda | \|f^{(j)} \|_u $ and $\|f+g\| = \Sigma _{j=0} ^k \|f^{(j)} + g^{(j)} \|_u \leq \Sigma_{j=0}^k \|f^{(j)}\|_u + \Sigma_{j=0}^k \|g^{(j)}\|_u = \|f+\|g\|$

Thus, we have shown this is a norm. Now we turn to show this is a Banach space.

Proceeding by induction, we consider the base case with $C^0([0,1])$ and the norm $\| f\|_u = \textrm{sup}_{[0,1]} |f|$. Let ${f_n}$ be a Cauchy sequence in $C^0([0,1])$. Then:

$\begin{split} \|f-f_n\|_u & = \textrm{sup}_{[0,1]}|f(x)-f_n(x)| \\ &= \textrm{sup}_{[0,1]} \textrm{lim}_{m \rightarrow \infty} |f_n(x)-f_m(x) | \\ & \leq \textrm{lim  inf}_{m \rightarrow \infty} \textrm{sup}_{[0,1]} | f_n(x)-f_m(x) | \\ &\rightarrow 0, \textrm{as  } n \rightarrow \infty  \end{split} $

Now let us suppose that this holds for $j \leq k$, we wish to show that this holds for $j=k+1$. Let ${f_n}$ be a Cauchy sequence in  $C^{k+1}([0,1])$. Then:

$\Sigma_0^k \|f_n^{(j)}-f_m^{j}\|_u < \Sigma_0^{k+1} \|f_n^{(j)}-f_m^{j}\|_u = \|f_n -f_m\| $

So then this is also a Cauchy sequence in $C^k([0,1])$ and converges to some $f \in C^k([0,1])$. So then our space is complete.

Exercise 10: Let $L^1_k([0,1])$ be the space of all $f \in C^{k-1}([0,1])$ such that $f^{(k-1)}$ is absolutely continuous on $[0,1]$. Then $\|f\| = \Sigma_0^k \int _0 ^1 | f^{(j)}(x)| dx$ is a norm on $L^1_k([0,1])$ that makes it into a Banach space.

Proof: First we check that this is norm. First and foremost, this is obviously nonnegative. Scalars can be removed safely as well. So we really just need to show that this satisfies the triangle inequality. That is, if $f,g \in L^1_k([0,1])$, then $\begin{split} \|x+y\|= \Sigma_0^k \int _0 ^1 | f^{(j)}(x) + g^{(j)}(x)| dx  & \leq  \Sigma_0^k(\int_0^1|f^{(j)}(x)|dx + \int_0^1|g^{(j)}(x)|dx) \\ &= \Sigma_0^k \int _0 ^1 | f^{(j)}(x)| dx + \Sigma_0^k \int _0 ^1 | g^{(j)}(x)| dx \\ &= \|x\| + \|y\|  \end{split}$
